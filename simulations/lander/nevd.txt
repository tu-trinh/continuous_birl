DOING WORLD 1
1 demonstrations
policy losses [('hypo67', -0.003931770525504287), ('crash', -0.0004766149948620055), ('hypo55', 0.0), ('anywhere', 0.00014532954578570616), ('center', 0.0027424892666154943)]
BOUND IS 0.0027424892666154943
2 demonstrations
policy losses [('hypo67', -0.003931770525504287), ('crash', -0.0004766149948620055), ('hypo55', 0.0), ('anywhere', 0.00014532954578570616), ('center', 0.0027424892666154943)]
BOUND IS 0.0027424892666154943
3 demonstrations
policy losses [('hypo67', -0.003931770525504287), ('crash', -0.0004766149948620055), ('hypo55', 0.0), ('anywhere', 0.00014532954578570616), ('center', 0.0027424892666154943)]
BOUND IS 0.0027424892666154943
4 demonstrations
policy losses [('hypo67', -0.003931770525504287), ('crash', -0.0004766149948620055), ('hypo55', 0.0), ('anywhere', 0.00014532954578570616), ('center', 0.0027424892666154943)]
BOUND IS 0.0027424892666154943
5 demonstrations
policy losses [('hypo67', -0.003931770525504287), ('crash', -0.0004766149948620055), ('hypo55', 0.0), ('anywhere', 0.00014532954578570616), ('center', 0.0027424892666154943)]
BOUND IS 0.0027424892666154943
NEW THRESHOLD 10
Policy loss bounds
0.0027424892666154943
0.0027424892666154943
0.0027424892666154943
0.0027424892666154943
0.0027424892666154943
Num demos
1
2
3
4
5
Percent states
0.2
0.4
0.6
0.8
1.0
True EVDs
0.0027424892666154943
0.0027424892666154943
0.0027424892666154943
0.0027424892666154943
0.0027424892666154943
Bound errors
0.0
0.0
0.0
0.0
0.0
Policy optimalities
1
1
1
1
1
Policy accuracies
0.2974137931034483
0.2974137931034483
0.2974137931034483
0.2974137931034483
0.2974137931034483
PMFs
[0.20044401 0.20045579 0.20013073 0.19784103 0.20112843]
[0.20087979 0.20096296 0.20031692 0.19557587 0.20226447]
[0.20131068 0.20145392 0.20048485 0.19334521 0.20340533]
[0.20173519 0.20192349 0.20062916 0.19118581 0.20452636]
[0.20233921 0.20173196 0.20014296 0.18985919 0.20592668]
Learned reward functions
hypo55
hypo55
hypo55
hypo55
hypo55
Confidence
1.0
Accuracy
1.0
Confusion matrices
[[5, 0], [0, 0]]
NEW THRESHOLD 20
Policy loss bounds
0.0027424892666154943
0.0027424892666154943
0.0027424892666154943
0.0027424892666154943
0.0027424892666154943
Num demos
1
2
3
4
5
Percent states
0.2
0.4
0.6
0.8
1.0
True EVDs
0.0027424892666154943
0.0027424892666154943
0.0027424892666154943
0.0027424892666154943
0.0027424892666154943
Bound errors
0.0
0.0
0.0
0.0
0.0
Policy optimalities
1
1
1
1
1
Policy accuracies
0.2974137931034483
0.2974137931034483
0.2974137931034483
0.2974137931034483
0.2974137931034483
PMFs
[0.20044401 0.20045579 0.20013073 0.19784103 0.20112843]
[0.20087979 0.20096296 0.20031692 0.19557587 0.20226447]
[0.20131068 0.20145392 0.20048485 0.19334521 0.20340533]
[0.20173519 0.20192349 0.20062916 0.19118581 0.20452636]
[0.20233921 0.20173196 0.20014296 0.18985919 0.20592668]
Learned reward functions
hypo55
hypo55
hypo55
hypo55
hypo55
Confidence
1.0
Accuracy
1.0
Confusion matrices
[[5, 0], [0, 0]]
NEW THRESHOLD 30
Policy loss bounds
0.0027424892666154943
0.0027424892666154943
0.0027424892666154943
0.0027424892666154943
0.0027424892666154943
Num demos
1
2
3
4
5
Percent states
0.2
0.4
0.6
0.8
1.0
True EVDs
0.0027424892666154943
0.0027424892666154943
0.0027424892666154943
0.0027424892666154943
0.0027424892666154943
Bound errors
0.0
0.0
0.0
0.0
0.0
Policy optimalities
1
1
1
1
1
Policy accuracies
0.2974137931034483
0.2974137931034483
0.2974137931034483
0.2974137931034483
0.2974137931034483
PMFs
[0.20044401 0.20045579 0.20013073 0.19784103 0.20112843]
[0.20087979 0.20096296 0.20031692 0.19557587 0.20226447]
[0.20131068 0.20145392 0.20048485 0.19334521 0.20340533]
[0.20173519 0.20192349 0.20062916 0.19118581 0.20452636]
[0.20233921 0.20173196 0.20014296 0.18985919 0.20592668]
Learned reward functions
hypo55
hypo55
hypo55
hypo55
hypo55
Confidence
1.0
Accuracy
1.0
Confusion matrices
[[5, 0], [0, 0]]
NEW THRESHOLD 40
Policy loss bounds
0.0027424892666154943
0.0027424892666154943
0.0027424892666154943
0.0027424892666154943
0.0027424892666154943
Num demos
1
2
3
4
5
Percent states
0.2
0.4
0.6
0.8
1.0
True EVDs
0.0027424892666154943
0.0027424892666154943
0.0027424892666154943
0.0027424892666154943
0.0027424892666154943
Bound errors
0.0
0.0
0.0
0.0
0.0
Policy optimalities
1
1
1
1
1
Policy accuracies
0.2974137931034483
0.2974137931034483
0.2974137931034483
0.2974137931034483
0.2974137931034483
PMFs
[0.20044401 0.20045579 0.20013073 0.19784103 0.20112843]
[0.20087979 0.20096296 0.20031692 0.19557587 0.20226447]
[0.20131068 0.20145392 0.20048485 0.19334521 0.20340533]
[0.20173519 0.20192349 0.20062916 0.19118581 0.20452636]
[0.20233921 0.20173196 0.20014296 0.18985919 0.20592668]
Learned reward functions
hypo55
hypo55
hypo55
hypo55
hypo55
Confidence
1.0
Accuracy
1.0
Confusion matrices
[[5, 0], [0, 0]]
NEW THRESHOLD 50
Policy loss bounds
0.0027424892666154943
0.0027424892666154943
0.0027424892666154943
0.0027424892666154943
0.0027424892666154943
Num demos
1
2
3
4
5
Percent states
0.2
0.4
0.6
0.8
1.0
True EVDs
0.0027424892666154943
0.0027424892666154943
0.0027424892666154943
0.0027424892666154943
0.0027424892666154943
Bound errors
0.0
0.0
0.0
0.0
0.0
Policy optimalities
1
1
1
1
1
Policy accuracies
0.2974137931034483
0.2974137931034483
0.2974137931034483
0.2974137931034483
0.2974137931034483
PMFs
[0.20044401 0.20045579 0.20013073 0.19784103 0.20112843]
[0.20087979 0.20096296 0.20031692 0.19557587 0.20226447]
[0.20131068 0.20145392 0.20048485 0.19334521 0.20340533]
[0.20173519 0.20192349 0.20062916 0.19118581 0.20452636]
[0.20233921 0.20173196 0.20014296 0.18985919 0.20592668]
Learned reward functions
hypo55
hypo55
hypo55
hypo55
hypo55
Confidence
1.0
Accuracy
1.0
Confusion matrices
[[5, 0], [0, 0]]
Reward hypotheses
center [0, 0, -100, 0, 0, -100, 0, -100, 0, 10, 10]
anywhere [0, 0, -200, 0, 0, -50, 0, -200, 0, 5, 5]
crash [0, 0, -300, 0, 0, -10, 0, -300, 0, 8, 8]
hypo67 [0, 0, -54, 0, 0, -131, 0, -127, 0, 17, 11]
hypo55 [0, 0, -111, 0, 0, -121, 0, -66, 0, 11, 11]
**************************************************
Time to run: 0.10830828348795572 minutes
